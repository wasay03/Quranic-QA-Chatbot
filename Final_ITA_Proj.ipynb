{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Code File is this one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Huzaifa Bin Tariq - 25133\n",
        "\n",
        "Hazim Ghulam Farooq - 25148\n",
        "\n",
        "Muhammad Wasay - 24497"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQv96DmCl6IU",
        "outputId": "3e0f20ec-2a27-4d43-9270-1c88f830f41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.3)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi==0.115.9 (from chromadb)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.25.0-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53801 sha256=23a91b8cd1e38e5ed59afd0b983c15cb101e1eba21e144d5ecea40ab39f5d769\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, build, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.5 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.1 opentelemetry-exporter-otlp-proto-common-1.32.1 opentelemetry-exporter-otlp-proto-grpc-1.32.1 opentelemetry-instrumentation-0.53b1 opentelemetry-instrumentation-asgi-0.53b1 opentelemetry-instrumentation-fastapi-0.53b1 opentelemetry-proto-1.32.1 opentelemetry-util-http-0.53b1 overrides-7.7.0 posthog-3.25.0 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.45.3 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5\n",
            "Collecting groq\n",
            "  Downloading groq-0.22.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
            "Downloading groq-0.22.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.22.0\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=536a2a85ca43e56f13e50b2be1435c53ce3ed39202f0dd04be84ff0f4b2c3ca8\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -q rank_bm25 faiss-cpu langchain transformers accelerate sentence-transformers pypdf langchain_community PyPDF2\n",
        "!pip install chromadb\n",
        "!pip install groq\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AkUqnqUtmIPf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma, FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from groq import Groq\n",
        "import os\n",
        "from langchain_core.language_models import BaseLLM\n",
        "from langchain_core.outputs import LLMResult\n",
        "from typing import Any, List, Optional, Mapping, Dict\n",
        "import chromadb\n",
        "from pydantic import Field\n",
        "from rouge_score import rouge_scorer\n",
        "from sentence_transformers import CrossEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGSitsmumSlA",
        "outputId": "852238b8-7405-4e4c-8a18-0db7001b7c1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: Who is Adam (A.S)?\n",
            "Processed: What is the significance of the 'forbidden tree' in the story of Adam (AS)?\n",
            "Processed: Why did the Jews of Madinah oppose Islam?\n",
            "Processed: What does the Qur'an say about the hypocrites (Munafiqun)?\n",
            "Processed: What lessons does the story of Adam (AS) teach about free will and repentance?\n",
            "Processed: How does the Qur'an use the Israelites' history as a warning for Muslims?\n",
            "✅ Ground truth generated using llma-8192 and saved to 'surah_baqarah_ground_truth_llama3.json'\n"
          ]
        }
      ],
      "source": [
        "from time import sleep\n",
        "\n",
        "# Configure Groq (replace with your API key)\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "client = Groq()\n",
        "\n",
        "# Questions to generate answers for\n",
        "questions = [\n",
        "    {\n",
        "        \"question\": \"Who is Adam (A.S)?\",\n",
        "        \"difficulty\": \"easy\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the significance of the 'forbidden tree' in the story of Adam (AS)?\",\n",
        "        \"difficulty\": \"easy\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Why did the Jews of Madinah oppose Islam?\",\n",
        "        \"difficulty\": \"medium\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What does the Qur'an say about the hypocrites (Munafiqun)?\",\n",
        "        \"difficulty\": \"medium\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What lessons does the story of Adam (AS) teach about free will and repentance?\",\n",
        "        \"difficulty\": \"hard\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does the Qur'an use the Israelites' history as a warning for Muslims?\",\n",
        "        \"difficulty\": \"hard\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# System prompt to enforce Surah Al-Baqarah-only answers\n",
        "system_prompt = \"\"\"You are a Quranic tafsir specialist. Answer with:\n",
        "1. **Exclusive focus**: Only use Surah Al-Baqarah (Chapter 2) as the source.\n",
        "2. **Verse citations**: Always reference ayat numbers (e.g., 2:143).\n",
        "3. **Classical tafsir**: Prioritize interpretations from Ibn Kathir/Tabari.\n",
        "4. **Rejection clause**: If the question cannot be answered from Surah Al-Baqarah, state: \"This is not addressed in Surah Al-Baqarah.\"\n",
        "\n",
        "Example format:\n",
        "Question: \"What is riba?\"\n",
        "Answer: \"Surah Al-Baqarah (2:275-279) prohibits riba (usury) and contrasts it with lawful trade...\"\n",
        "\"\"\"\n",
        "\n",
        "# Generate answers using deepseek r1\n",
        "ground_truth_qa = []\n",
        "\n",
        "for q in questions:\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": q[\"question\"]}\n",
        "            ],\n",
        "            model=\"llama3-70b-8192\",  # More accurate than Mixtral for tafsir\n",
        "            temperature=0.2,  # Lower = more factual\n",
        "            max_tokens=600   # Allows detailed explanations with verses\n",
        "        )\n",
        "\n",
        "        answer = response.choices[0].message.content\n",
        "\n",
        "        # Post-processing to ensure compliance\n",
        "        if \"Surah Al-Baqarah\" not in answer and \"2:\" not in answer:\n",
        "            answer = \"[RAGAS FILTERED] This answer could not be verified from Surah Al-Baqarah. Original response: \" + answer\n",
        "\n",
        "        ground_truth_qa.append({\n",
        "            \"question\": q[\"question\"],\n",
        "            \"answer\": answer,\n",
        "            \"difficulty\": q[\"difficulty\"]\n",
        "        })\n",
        "\n",
        "        print(f\"Processed: {q['question']}\")\n",
        "        sleep(1.5)  # Avoid rate limits\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error on '{q['question']}': {str(e)}\")\n",
        "        ground_truth_qa.append({\n",
        "            \"question\": q[\"question\"],\n",
        "            \"answer\": f\"[ERROR] Failed to generate answer: {str(e)}\",\n",
        "            \"difficulty\": q[\"difficulty\"]\n",
        "        })\n",
        "\n",
        "# Save results\n",
        "with open(\"surah_baqarah_ground_truth.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(ground_truth_qa, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"✅ Ground truth generated using llma-8192 and saved to 'surah_baqarah_ground_truth.json'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnIsnwYo0hVR",
        "outputId": "3f7f494f-f98c-40c4-85f1-692dd3eee2a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bert_score\n",
            "Successfully installed bert_score-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ueI45VF0nk1t"
      },
      "outputs": [],
      "source": [
        "from transformers import logging\n",
        "logging.set_verbosity_error()  # Add this before importing/using BERTScore\n",
        "\n",
        "from bert_score import score\n",
        "from bert_score import score\n",
        "# Custom JSON encoder to handle numpy float32\n",
        "class NumpyFloatValuesEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.float32):\n",
        "            return float(obj)\n",
        "        return super().default(obj)\n",
        "\n",
        "# --- Set your Groq API key here ---\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "# Initialize cross-encoder for faithfulness scoring\n",
        "cross_encoder = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
        "\n",
        "# Initialize ROUGE scorer for relevance\n",
        "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def load_pdf(file_path):\n",
        "    \"\"\"Load PDF file and extract text.\"\"\"\n",
        "    text = \"\"\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        reader = PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def load_ground_truth(file_path):\n",
        "    \"\"\"Load ground truth Q&A.\"\"\"\n",
        "    with open(file_path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def calculate_semantic_similarity(text1, text2, embedding_model):\n",
        "    \"\"\"Calculate cosine similarity between embeddings of two texts.\"\"\"\n",
        "    embedding1 = embedding_model.embed_query(text1)\n",
        "    embedding2 = embedding_model.embed_query(text2)\n",
        "    return float(cosine_similarity([embedding1], [embedding2])[0][0])  # Convert to Python float\n",
        "\n",
        "def calculate_relevance_score(answer, context):\n",
        "    \"\"\"Calculate relevance using ROUGE scores between answer and context.\"\"\"\n",
        "    scores = rouge.score(answer, \" \".join(context))\n",
        "    return float((scores['rouge1'].fmeasure + scores['rouge2'].fmeasure + scores['rougeL'].fmeasure) / 3)\n",
        "    # _, _, f1 = score([answer], [context], lang='en')\n",
        "    # return float(f1.mean())\n",
        "\n",
        "def calculate_faithfulness_score(answer, ground_truth):\n",
        "    \"\"\"Calculate faithfulness using cross-encoder between answer and ground truth.\"\"\"\n",
        "    return float(cross_encoder.predict([[answer, ground_truth]])[0])\n",
        "\n",
        "def reciprocal_rank(relevant_doc_positions):\n",
        "    \"\"\"Calculate Reciprocal Rank (RR).\"\"\"\n",
        "    if not relevant_doc_positions:\n",
        "        return 0.0\n",
        "    return float(1.0 / (relevant_doc_positions[0] + 1))  # Convert to Python float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bXdWI8tlnl2T"
      },
      "outputs": [],
      "source": [
        "# --- LLM Prompt Template ---\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based ONLY on the following context. If unsure, say \"I don't know\".\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "class GroqLLM(BaseLLM):\n",
        "    model_name: str = Field(default=\"meta-llama/llama-4-maverick-17b-128e-instruct\")\n",
        "    temperature: float = Field(default=0.3)\n",
        "    max_tokens: int = Field(default=4000)\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        try:\n",
        "            response = self._client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompts[0]}],\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=self.max_tokens,\n",
        "                stop=stop\n",
        "            )\n",
        "            return LLMResult(\n",
        "                generations=[[{\"text\": response.choices[0].message.content}]]\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling Groq API: {str(e)}\")\n",
        "            return LLMResult(generations=[[{\"text\": \"I don't know\"}]])\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"groq\"\n",
        "\n",
        "# --- RAG Pipeline Builder ---\n",
        "def build_rag_pipeline(texts, config, embedding):\n",
        "    \"\"\"Build RAG pipeline with specified config.\"\"\"\n",
        "    # Vector Store\n",
        "    if config[\"vector_store\"] == \"Chroma\":\n",
        "        # Clear existing Chroma collection when changing embedding models\n",
        "        client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "        try:\n",
        "            # Delete existing collection if it exists\n",
        "            client.delete_collection(\"rag_collection\")\n",
        "        except:\n",
        "            pass  # Collection didn't exist\n",
        "\n",
        "        # Create new collection with current embedding\n",
        "        vectorstore = Chroma.from_texts(\n",
        "            texts=texts,\n",
        "            embedding=embedding,\n",
        "            client=client,\n",
        "            collection_name=\"rag_collection\"\n",
        "        )\n",
        "    else:  # FAISS\n",
        "        vectorstore = FAISS.from_texts(texts, embedding)\n",
        "\n",
        "    # Retrievers\n",
        "    if config[\"ranking\"] == \"BM25\":\n",
        "        retriever = BM25Retriever.from_texts(texts, k=4)\n",
        "    elif config[\"ranking\"] == \"dense\":\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "    else:  # Hybrid\n",
        "        bm25_retriever = BM25Retriever.from_texts(texts, k=4)\n",
        "        dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "        retriever = EnsembleRetriever(\n",
        "            retrievers=[bm25_retriever, dense_retriever],\n",
        "            weights=[0.5, 0.5],\n",
        "            c=60\n",
        "        )\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=PROMPT_TEMPLATE,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    llm = GroqLLM(\n",
        "        model_name=config[\"llm\"],\n",
        "        temperature=0.3,\n",
        "        max_tokens=4000\n",
        "    )\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    return qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3k4zZNlnoQM",
        "outputId": "27c8ec3b-ef74-4d3f-ccb3-51d4a7a61c7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing: {'chunk_size': 500, 'chunk_overlap': 100, 'vector_store': 'Chroma', 'query_expansion': False, 'embedding': 'sentence-transformers/all-MiniLM-L6-v2', 'llm': 'llama3-8b-8192', 'ranking': 'BM25', 'search_type': 'semantic', 'processing': None}\n",
            "Results:\n",
            "- Time: 0.39s\n",
            "- Semantic Similarity: 0.62\n",
            "- Relevance: 0.12\n",
            "- Faithfulness: 0.56\n",
            "\n",
            "Testing: {'chunk_size': 500, 'chunk_overlap': 100, 'vector_store': 'FAISS', 'query_expansion': True, 'embedding': 'sentence-transformers/all-MiniLM-L6-v2', 'llm': 'qwen-qwq-32b', 'ranking': 'dense', 'search_type': 'keyword', 'processing': 'summarization'}\n",
            "Results:\n",
            "- Time: 1.58s\n",
            "- Semantic Similarity: 0.72\n",
            "- Relevance: 0.29\n",
            "- Faithfulness: 0.62\n",
            "\n",
            "Testing: {'chunk_size': 500, 'chunk_overlap': 100, 'vector_store': 'Chroma', 'query_expansion': True, 'embedding': 'sentence-transformers/all-MiniLM-L6-v2', 'llm': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'ranking': 'hybrid', 'search_type': 'semantic', 'processing': 'long_context_reordering'}\n",
            "Results:\n",
            "- Time: 0.86s\n",
            "- Semantic Similarity: 0.74\n",
            "- Relevance: 0.20\n",
            "- Faithfulness: 0.64\n",
            "\n",
            "Testing: {'chunk_size': 500, 'chunk_overlap': 100, 'vector_store': 'FAISS', 'query_expansion': False, 'embedding': 'BAAI/bge-base-en', 'llm': 'llama3-8b-8192', 'ranking': 'BM25', 'search_type': 'semantic', 'processing': None}\n",
            "Results:\n",
            "- Time: 0.78s\n",
            "- Semantic Similarity: 0.86\n",
            "- Relevance: 0.11\n",
            "- Faithfulness: 0.54\n",
            "\n",
            "Testing: {'chunk_size': 500, 'chunk_overlap': 100, 'vector_store': 'Chroma', 'query_expansion': True, 'embedding': 'BAAI/bge-base-en', 'llm': 'qwen-qwq-32b', 'ranking': 'dense', 'search_type': 'keyword', 'processing': 'summarization'}\n",
            "Results:\n",
            "- Time: 1.99s\n",
            "- Semantic Similarity: 0.90\n",
            "- Relevance: 0.31\n",
            "- Faithfulness: 0.65\n",
            "\n",
            "Testing: {'chunk_size': 500, 'chunk_overlap': 100, 'vector_store': 'FAISS', 'query_expansion': True, 'embedding': 'BAAI/bge-base-en', 'llm': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'ranking': 'hybrid', 'search_type': 'keyword', 'processing': 'long_context_reordering'}\n",
            "Results:\n",
            "- Time: 0.85s\n",
            "- Semantic Similarity: 0.91\n",
            "- Relevance: 0.19\n",
            "- Faithfulness: 0.62\n",
            "\n",
            "Testing: {'chunk_size': 500, 'chunk_overlap': 100, 'vector_store': 'Chroma', 'query_expansion': False, 'embedding': 'intfloat/e5-base-v2', 'llm': 'llama3-8b-8192', 'ranking': 'BM25', 'search_type': 'semantic', 'processing': None}\n",
            "Results:\n",
            "- Time: 0.40s\n",
            "- Semantic Similarity: 0.86\n",
            "- Relevance: 0.12\n",
            "- Faithfulness: 0.54\n",
            "\n",
            "Testing: {'chunk_size': 500, 'chunk_overlap': 100, 'vector_store': 'FAISS', 'query_expansion': True, 'embedding': 'intfloat/e5-base-v2', 'llm': 'qwen-qwq-32b', 'ranking': 'dense', 'search_type': 'keyword', 'processing': 'summarization'}\n",
            "Results:\n",
            "- Time: 1.84s\n",
            "- Semantic Similarity: 0.86\n",
            "- Relevance: 0.30\n",
            "- Faithfulness: 0.63\n",
            "\n",
            "Testing: {'chunk_size': 500, 'chunk_overlap': 100, 'vector_store': 'Chroma', 'query_expansion': True, 'embedding': 'intfloat/e5-base-v2', 'llm': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'ranking': 'hybrid', 'search_type': 'semantic', 'processing': 'long_context_reordering'}\n",
            "Results:\n",
            "- Time: 0.66s\n",
            "- Semantic Similarity: 0.87\n",
            "- Relevance: 0.18\n",
            "- Faithfulness: 0.60\n",
            "\n",
            "✅ Results saved to 'rag_config_results.json'\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration Setup ---\n",
        "CONFIGS = [\n",
        "    # 500-100 chunks\n",
        "    {\n",
        "        \"chunk_size\": 500,\n",
        "        \"chunk_overlap\": 100,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"llm\": \"llama3-8b-8192\",\n",
        "        \"ranking\": \"BM25\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": None\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 500,\n",
        "        \"chunk_overlap\": 100,\n",
        "        \"vector_store\": \"FAISS\",\n",
        "\n",
        "        \"embedding\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"llm\": \"qwen-qwq-32b\",\n",
        "        \"ranking\": \"dense\",\n",
        "        \"search_type\": \"keyword\",\n",
        "        \"processing\": \"summarization\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 500,\n",
        "        \"chunk_overlap\": 100,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"llm\": \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
        "        \"ranking\": \"hybrid\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": \"long_context_reordering\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 500,\n",
        "        \"chunk_overlap\": 100,\n",
        "        \"vector_store\": \"FAISS\",\n",
        "\n",
        "        \"embedding\": \"BAAI/bge-base-en\",\n",
        "        \"llm\": \"llama3-8b-8192\",\n",
        "        \"ranking\": \"BM25\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": None\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 500,\n",
        "        \"chunk_overlap\": 100,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"BAAI/bge-base-en\",\n",
        "        \"llm\": \"qwen-qwq-32b\",\n",
        "        \"ranking\": \"dense\",\n",
        "        \"search_type\": \"keyword\",\n",
        "        \"processing\": \"summarization\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 500,\n",
        "        \"chunk_overlap\": 100,\n",
        "        \"vector_store\": \"FAISS\",\n",
        "\n",
        "        \"embedding\": \"BAAI/bge-base-en\",\n",
        "        \"llm\": \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
        "        \"ranking\": \"hybrid\",\n",
        "        \"search_type\": \"keyword\",\n",
        "        \"processing\": \"long_context_reordering\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 500,\n",
        "        \"chunk_overlap\": 100,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"intfloat/e5-base-v2\",\n",
        "        \"llm\": \"llama3-8b-8192\",\n",
        "        \"ranking\": \"BM25\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": None\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 500,\n",
        "        \"chunk_overlap\": 100,\n",
        "        \"vector_store\": \"FAISS\",\n",
        "\n",
        "        \"embedding\": \"intfloat/e5-base-v2\",\n",
        "        \"llm\": \"qwen-qwq-32b\",\n",
        "        \"ranking\": \"dense\",\n",
        "        \"search_type\": \"keyword\",\n",
        "        \"processing\": \"summarization\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 500,\n",
        "        \"chunk_overlap\": 100,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"intfloat/e5-base-v2\",\n",
        "        \"llm\": \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
        "        \"ranking\": \"hybrid\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": \"long_context_reordering\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# --- Evaluation Function ---\n",
        "def evaluate_config(config, questions, ground_truths, embedding_model):\n",
        "    \"\"\"Evaluate a single RAG configuration.\"\"\"\n",
        "    text = load_pdf(\"/content/Surah Baqarah tafsir english (2).pdf\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=config[\"chunk_size\"],\n",
        "        chunk_overlap=config[\"chunk_overlap\"]\n",
        "    )\n",
        "    texts = text_splitter.split_text(text)\n",
        "\n",
        "    embedding = HuggingFaceEmbeddings(model_name=config[\"embedding\"])\n",
        "\n",
        "    start_time = time.time()\n",
        "    qa_chain = build_rag_pipeline(texts, config, embedding)\n",
        "    pipeline_time = time.time() - start_time\n",
        "\n",
        "    results = []\n",
        "    for q_idx, question in enumerate(questions):\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            result = qa_chain.invoke({\"query\": question})\n",
        "            time_taken = time.time() - start_time\n",
        "\n",
        "            answer = result[\"result\"]\n",
        "            retrieved_docs = result[\"source_documents\"][:4]\n",
        "            doc_contents = [doc.page_content for doc in retrieved_docs]\n",
        "            gt_answer = ground_truths[q_idx][\"answer\"]\n",
        "\n",
        "            # Calculate all metrics\n",
        "            semantic_sim = calculate_semantic_similarity(gt_answer, answer, embedding_model)\n",
        "            relevance = calculate_relevance_score(answer, doc_contents)\n",
        "            faithfulness = calculate_faithfulness_score(answer, gt_answer)\n",
        "\n",
        "            results.append({\n",
        "                \"question\": question,\n",
        "                \"ground_truth\": gt_answer,\n",
        "                \"generated_answer\": answer,\n",
        "                \"retrieved_docs\": doc_contents,\n",
        "                \"time_taken\": float(time_taken),\n",
        "                \"semantic_similarity\": semantic_sim,\n",
        "                \"relevance_score\": relevance,\n",
        "                \"faithfulness_score\": faithfulness,\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {question}\")\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Aggregate metrics\n",
        "    if results:\n",
        "        avg_metrics = {\n",
        "            \"time_taken\": float(np.mean([r[\"time_taken\"] for r in results])),\n",
        "            \"semantic_similarity\": float(np.mean([r[\"semantic_similarity\"] for r in results])),\n",
        "            \"relevance_score\": float(np.mean([r[\"relevance_score\"] for r in results])),\n",
        "            \"faithfulness_score\": float(np.mean([r[\"faithfulness_score\"] for r in results])),\n",
        "        }\n",
        "    else:\n",
        "        avg_metrics = {\n",
        "            \"time_taken\": 0.0,\n",
        "            \"semantic_similarity\": 0.0,\n",
        "            \"relevance_score\": 0.0,\n",
        "            \"faithfulness_score\": 0.0,\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"config\": config,\n",
        "        \"avg_metrics\": avg_metrics,\n",
        "        \"results\": results\n",
        "    }\n",
        "\n",
        "# --- Main Execution ---\n",
        "# Load data once\n",
        "ground_truths = load_ground_truth(\"surah_baqarah_ground_truth.json\")\n",
        "questions = [q[\"question\"] for q in ground_truths]\n",
        "\n",
        "# Run evaluations\n",
        "all_results = []\n",
        "for config in CONFIGS:\n",
        "    print(f\"\\nTesting: {config}\")\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=config[\"embedding\"])\n",
        "    result = evaluate_config(config, questions, ground_truths, embedding_model)\n",
        "    all_results.append(result)\n",
        "\n",
        "    metrics = result[\"avg_metrics\"]\n",
        "    print(f\"Results:\")\n",
        "    print(f\"- Time: {metrics['time_taken']:.2f}s\")\n",
        "    print(f\"- Semantic Similarity: {metrics['semantic_similarity']:.2f}\")\n",
        "    print(f\"- Relevance: {metrics['relevance_score']:.2f}\")\n",
        "    print(f\"- Faithfulness: {metrics['faithfulness_score']:.2f}\")\n",
        "\n",
        "# Save results\n",
        "with open(\"rag_config_results_1.json\", \"w\") as f:\n",
        "    json.dump(all_results, f, indent=4, cls=NumpyFloatValuesEncoder)\n",
        "print(\"\\n✅ Results saved to 'rag_config_results.json'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MR171TMnraB",
        "outputId": "48879ea1-00a6-472f-e728-24c2bf217bbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing: {'chunk_size': 1000, 'chunk_overlap': 200, 'vector_store': 'Chroma', 'query_expansion': False, 'embedding': 'sentence-transformers/all-MiniLM-L6-v2', 'llm': 'llama3-8b-8192', 'ranking': 'BM25', 'search_type': 'semantic', 'processing': None}\n",
            "Results:\n",
            "- Time: 0.71s\n",
            "- Semantic Similarity: 0.67\n",
            "- Relevance: 0.08\n",
            "- Faithfulness: 0.56\n",
            "\n",
            "Testing: {'chunk_size': 1000, 'chunk_overlap': 200, 'vector_store': 'FAISS', 'query_expansion': True, 'embedding': 'sentence-transformers/all-MiniLM-L6-v2', 'llm': 'qwen-qwq-32b', 'ranking': 'dense', 'search_type': 'keyword', 'processing': 'summarization'}\n",
            "Results:\n",
            "- Time: 4.78s\n",
            "- Semantic Similarity: 0.71\n",
            "- Relevance: 0.26\n",
            "- Faithfulness: 0.66\n",
            "\n",
            "Testing: {'chunk_size': 1000, 'chunk_overlap': 200, 'vector_store': 'Chroma', 'query_expansion': True, 'embedding': 'sentence-transformers/all-MiniLM-L6-v2', 'llm': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'ranking': 'hybrid', 'search_type': 'semantic', 'processing': 'long_context_reordering'}\n",
            "Results:\n",
            "- Time: 3.54s\n",
            "- Semantic Similarity: 0.72\n",
            "- Relevance: 0.14\n",
            "- Faithfulness: 0.59\n",
            "\n",
            "Testing: {'chunk_size': 1000, 'chunk_overlap': 200, 'vector_store': 'FAISS', 'query_expansion': False, 'embedding': 'BAAI/bge-base-en', 'llm': 'llama3-8b-8192', 'ranking': 'BM25', 'search_type': 'semantic', 'processing': None}\n",
            "Results:\n",
            "- Time: 0.57s\n",
            "- Semantic Similarity: 0.89\n",
            "- Relevance: 0.08\n",
            "- Faithfulness: 0.57\n",
            "\n",
            "Testing: {'chunk_size': 1000, 'chunk_overlap': 200, 'vector_store': 'Chroma', 'query_expansion': True, 'embedding': 'BAAI/bge-base-en', 'llm': 'qwen-qwq-32b', 'ranking': 'dense', 'search_type': 'keyword', 'processing': 'summarization'}\n",
            "Results:\n",
            "- Time: 3.10s\n",
            "- Semantic Similarity: 0.90\n",
            "- Relevance: 0.27\n",
            "- Faithfulness: 0.62\n",
            "\n",
            "Testing: {'chunk_size': 1000, 'chunk_overlap': 200, 'vector_store': 'FAISS', 'query_expansion': True, 'embedding': 'BAAI/bge-base-en', 'llm': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'ranking': 'hybrid', 'search_type': 'keyword', 'processing': 'long_context_reordering'}\n",
            "Results:\n",
            "- Time: 3.00s\n",
            "- Semantic Similarity: 0.92\n",
            "- Relevance: 0.13\n",
            "- Faithfulness: 0.65\n",
            "\n",
            "Testing: {'chunk_size': 1000, 'chunk_overlap': 200, 'vector_store': 'Chroma', 'query_expansion': False, 'embedding': 'intfloat/e5-base-v2', 'llm': 'llama3-8b-8192', 'ranking': 'BM25', 'search_type': 'semantic', 'processing': None}\n",
            "Results:\n",
            "- Time: 0.39s\n",
            "- Semantic Similarity: 0.86\n",
            "- Relevance: 0.08\n",
            "- Faithfulness: 0.57\n",
            "\n",
            "Testing: {'chunk_size': 1000, 'chunk_overlap': 200, 'vector_store': 'FAISS', 'query_expansion': True, 'embedding': 'intfloat/e5-base-v2', 'llm': 'qwen-qwq-32b', 'ranking': 'dense', 'search_type': 'keyword', 'processing': 'summarization'}\n",
            "Results:\n",
            "- Time: 4.92s\n",
            "- Semantic Similarity: 0.87\n",
            "- Relevance: 0.25\n",
            "- Faithfulness: 0.64\n",
            "\n",
            "Testing: {'chunk_size': 1000, 'chunk_overlap': 200, 'vector_store': 'Chroma', 'query_expansion': True, 'embedding': 'intfloat/e5-base-v2', 'llm': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'ranking': 'hybrid', 'search_type': 'semantic', 'processing': 'long_context_reordering'}\n",
            "Results:\n",
            "- Time: 8.50s\n",
            "- Semantic Similarity: 0.88\n",
            "- Relevance: 0.13\n",
            "- Faithfulness: 0.63\n",
            "\n",
            "✅ Results saved to 'rag_config_results.json'\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration Setup ---\n",
        "CONFIGS = [\n",
        "    # 1000-200 chunks\n",
        "    {\n",
        "        \"chunk_size\": 1000,\n",
        "        \"chunk_overlap\": 200,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"llm\": \"llama3-8b-8192\",\n",
        "        \"ranking\": \"BM25\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": None\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1000,\n",
        "        \"chunk_overlap\": 200,\n",
        "        \"vector_store\": \"FAISS\",\n",
        "\n",
        "        \"embedding\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"llm\": \"qwen-qwq-32b\",\n",
        "        \"ranking\": \"dense\",\n",
        "        \"search_type\": \"keyword\",\n",
        "        \"processing\": \"summarization\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1000,\n",
        "        \"chunk_overlap\": 200,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"llm\": \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
        "        \"ranking\": \"hybrid\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": \"long_context_reordering\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1000,\n",
        "        \"chunk_overlap\": 200,\n",
        "        \"vector_store\": \"FAISS\",\n",
        "\n",
        "        \"embedding\": \"BAAI/bge-base-en\",\n",
        "        \"llm\": \"llama3-8b-8192\",\n",
        "        \"ranking\": \"BM25\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": None\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1000,\n",
        "        \"chunk_overlap\": 200,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"BAAI/bge-base-en\",\n",
        "        \"llm\": \"qwen-qwq-32b\",\n",
        "        \"ranking\": \"dense\",\n",
        "        \"search_type\": \"keyword\",\n",
        "        \"processing\": \"summarization\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1000,\n",
        "        \"chunk_overlap\": 200,\n",
        "        \"vector_store\": \"FAISS\",\n",
        "\n",
        "        \"embedding\": \"BAAI/bge-base-en\",\n",
        "        \"llm\": \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
        "        \"ranking\": \"hybrid\",\n",
        "        \"search_type\": \"keyword\",\n",
        "        \"processing\": \"long_context_reordering\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1000,\n",
        "        \"chunk_overlap\": 200,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"intfloat/e5-base-v2\",\n",
        "        \"llm\": \"llama3-8b-8192\",\n",
        "        \"ranking\": \"BM25\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": None\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1000,\n",
        "        \"chunk_overlap\": 200,\n",
        "        \"vector_store\": \"FAISS\",\n",
        "\n",
        "        \"embedding\": \"intfloat/e5-base-v2\",\n",
        "        \"llm\": \"qwen-qwq-32b\",\n",
        "        \"ranking\": \"dense\",\n",
        "        \"search_type\": \"keyword\",\n",
        "        \"processing\": \"summarization\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1000,\n",
        "        \"chunk_overlap\": 200,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"intfloat/e5-base-v2\",\n",
        "        \"llm\": \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
        "        \"ranking\": \"hybrid\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": \"long_context_reordering\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# --- Evaluation Function ---\n",
        "def evaluate_config(config, questions, ground_truths, embedding_model):\n",
        "    \"\"\"Evaluate a single RAG configuration.\"\"\"\n",
        "    text = load_pdf(\"/content/Surah Baqarah tafsir english (2).pdf\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=config[\"chunk_size\"],\n",
        "        chunk_overlap=config[\"chunk_overlap\"]\n",
        "    )\n",
        "    texts = text_splitter.split_text(text)\n",
        "\n",
        "    embedding = HuggingFaceEmbeddings(model_name=config[\"embedding\"])\n",
        "\n",
        "    start_time = time.time()\n",
        "    qa_chain = build_rag_pipeline(texts, config, embedding)\n",
        "    pipeline_time = time.time() - start_time\n",
        "\n",
        "    results = []\n",
        "    for q_idx, question in enumerate(questions):\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            result = qa_chain.invoke({\"query\": question})\n",
        "            time_taken = time.time() - start_time\n",
        "\n",
        "            answer = result[\"result\"]\n",
        "            retrieved_docs = result[\"source_documents\"][:4]\n",
        "            doc_contents = [doc.page_content for doc in retrieved_docs]\n",
        "            gt_answer = ground_truths[q_idx][\"answer\"]\n",
        "\n",
        "            # Calculate all metrics\n",
        "            semantic_sim = calculate_semantic_similarity(gt_answer, answer, embedding_model)\n",
        "            relevance = calculate_relevance_score(answer, doc_contents)\n",
        "            faithfulness = calculate_faithfulness_score(answer, gt_answer)\n",
        "\n",
        "            results.append({\n",
        "                \"question\": question,\n",
        "                \"ground_truth\": gt_answer,\n",
        "                \"generated_answer\": answer,\n",
        "                \"retrieved_docs\": doc_contents,\n",
        "                \"time_taken\": float(time_taken),\n",
        "                \"semantic_similarity\": semantic_sim,\n",
        "                \"relevance_score\": relevance,\n",
        "                \"faithfulness_score\": faithfulness,\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {question}\")\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Aggregate metrics\n",
        "    if results:\n",
        "        avg_metrics = {\n",
        "            \"time_taken\": float(np.mean([r[\"time_taken\"] for r in results])),\n",
        "            \"semantic_similarity\": float(np.mean([r[\"semantic_similarity\"] for r in results])),\n",
        "            \"relevance_score\": float(np.mean([r[\"relevance_score\"] for r in results])),\n",
        "            \"faithfulness_score\": float(np.mean([r[\"faithfulness_score\"] for r in results])),\n",
        "        }\n",
        "    else:\n",
        "        avg_metrics = {\n",
        "            \"time_taken\": 0.0,\n",
        "            \"semantic_similarity\": 0.0,\n",
        "            \"relevance_score\": 0.0,\n",
        "            \"faithfulness_score\": 0.0,\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"config\": config,\n",
        "        \"avg_metrics\": avg_metrics,\n",
        "        \"results\": results\n",
        "    }\n",
        "\n",
        "# --- Main Execution ---\n",
        "# Load data once\n",
        "ground_truths = load_ground_truth(\"surah_baqarah_ground_truth.json\")\n",
        "questions = [q[\"question\"] for q in ground_truths]\n",
        "\n",
        "# Run evaluations\n",
        "all_results = []\n",
        "for config in CONFIGS:\n",
        "    print(f\"\\nTesting: {config}\")\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=config[\"embedding\"])\n",
        "    result = evaluate_config(config, questions, ground_truths, embedding_model)\n",
        "    all_results.append(result)\n",
        "\n",
        "    metrics = result[\"avg_metrics\"]\n",
        "    print(f\"Results:\")\n",
        "    print(f\"- Time: {metrics['time_taken']:.2f}s\")\n",
        "    print(f\"- Semantic Similarity: {metrics['semantic_similarity']:.2f}\")\n",
        "    print(f\"- Relevance: {metrics['relevance_score']:.2f}\")\n",
        "    print(f\"- Faithfulness: {metrics['faithfulness_score']:.2f}\")\n",
        "\n",
        "# Save results\n",
        "with open(\"rag_config_results_2.json\", \"w\") as f:\n",
        "    json.dump(all_results, f, indent=4, cls=NumpyFloatValuesEncoder)\n",
        "print(\"\\n✅ Results saved to 'rag_config_results.json'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os4kPdXnnt2v",
        "outputId": "43fdbe02-5c54-47fd-ed6b-a23688299a8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing: {'chunk_size': 1500, 'chunk_overlap': 300, 'vector_store': 'Chroma', 'query_expansion': False, 'embedding': 'sentence-transformers/all-MiniLM-L6-v2', 'llm': 'llama3-8b-8192', 'ranking': 'BM25', 'search_type': 'semantic', 'processing': None}\n",
            "Results:\n",
            "- Time: 4.71s\n",
            "- Semantic Similarity: 0.72\n",
            "- Relevance: 0.07\n",
            "- Faithfulness: 0.58\n",
            "\n",
            "Testing: {'chunk_size': 1500, 'chunk_overlap': 300, 'vector_store': 'FAISS', 'query_expansion': True, 'embedding': 'sentence-transformers/all-MiniLM-L6-v2', 'llm': 'qwen-qwq-32b', 'ranking': 'dense', 'search_type': 'keyword', 'processing': 'summarization'}\n",
            "Results:\n",
            "- Time: 3.53s\n",
            "- Semantic Similarity: 0.69\n",
            "- Relevance: 0.24\n",
            "- Faithfulness: 0.62\n",
            "\n",
            "Testing: {'chunk_size': 1500, 'chunk_overlap': 300, 'vector_store': 'Chroma', 'query_expansion': True, 'embedding': 'sentence-transformers/all-MiniLM-L6-v2', 'llm': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'ranking': 'hybrid', 'search_type': 'semantic', 'processing': 'long_context_reordering'}\n",
            "Results:\n",
            "- Time: 12.24s\n",
            "- Semantic Similarity: 0.77\n",
            "- Relevance: 0.11\n",
            "- Faithfulness: 0.64\n",
            "\n",
            "Testing: {'chunk_size': 1500, 'chunk_overlap': 300, 'vector_store': 'FAISS', 'query_expansion': False, 'embedding': 'BAAI/bge-base-en', 'llm': 'llama3-8b-8192', 'ranking': 'BM25', 'search_type': 'semantic', 'processing': None}\n",
            "Results:\n",
            "- Time: 4.81s\n",
            "- Semantic Similarity: 0.89\n",
            "- Relevance: 0.07\n",
            "- Faithfulness: 0.58\n",
            "\n",
            "Testing: {'chunk_size': 1500, 'chunk_overlap': 300, 'vector_store': 'Chroma', 'query_expansion': True, 'embedding': 'BAAI/bge-base-en', 'llm': 'qwen-qwq-32b', 'ranking': 'dense', 'search_type': 'keyword', 'processing': 'summarization'}\n",
            "Results:\n",
            "- Time: 10.01s\n",
            "- Semantic Similarity: 0.91\n",
            "- Relevance: 0.23\n",
            "- Faithfulness: 0.62\n",
            "\n",
            "Testing: {'chunk_size': 1500, 'chunk_overlap': 300, 'vector_store': 'FAISS', 'query_expansion': True, 'embedding': 'BAAI/bge-base-en', 'llm': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'ranking': 'hybrid', 'search_type': 'keyword', 'processing': 'long_context_reordering'}\n",
            "Results:\n",
            "- Time: 12.81s\n",
            "- Semantic Similarity: 0.92\n",
            "- Relevance: 0.09\n",
            "- Faithfulness: 0.63\n",
            "\n",
            "Testing: {'chunk_size': 1500, 'chunk_overlap': 300, 'vector_store': 'Chroma', 'query_expansion': False, 'embedding': 'intfloat/e5-base-v2', 'llm': 'llama3-8b-8192', 'ranking': 'BM25', 'search_type': 'semantic', 'processing': None}\n",
            "Results:\n",
            "- Time: 2.43s\n",
            "- Semantic Similarity: 0.87\n",
            "- Relevance: 0.07\n",
            "- Faithfulness: 0.58\n",
            "\n",
            "Testing: {'chunk_size': 1500, 'chunk_overlap': 300, 'vector_store': 'FAISS', 'query_expansion': True, 'embedding': 'intfloat/e5-base-v2', 'llm': 'qwen-qwq-32b', 'ranking': 'dense', 'search_type': 'keyword', 'processing': 'summarization'}\n",
            "Results:\n",
            "- Time: 7.81s\n",
            "- Semantic Similarity: 0.87\n",
            "- Relevance: 0.23\n",
            "- Faithfulness: 0.64\n",
            "\n",
            "Testing: {'chunk_size': 1500, 'chunk_overlap': 300, 'vector_store': 'Chroma', 'query_expansion': True, 'embedding': 'intfloat/e5-base-v2', 'llm': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'ranking': 'hybrid', 'search_type': 'semantic', 'processing': 'long_context_reordering'}\n",
            "Results:\n",
            "- Time: 9.23s\n",
            "- Semantic Similarity: 0.88\n",
            "- Relevance: 0.08\n",
            "- Faithfulness: 0.63\n",
            "\n",
            "✅ Results saved to 'rag_config_results.json'\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration Setup ---\n",
        "CONFIGS = [\n",
        "    # 1500-300 chunks\n",
        "    {\n",
        "        \"chunk_size\": 1500,\n",
        "        \"chunk_overlap\": 300,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"llm\": \"llama3-8b-8192\",\n",
        "        \"ranking\": \"BM25\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": None\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1500,\n",
        "        \"chunk_overlap\": 300,\n",
        "        \"vector_store\": \"FAISS\",\n",
        "\n",
        "        \"embedding\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"llm\": \"qwen-qwq-32b\",\n",
        "        \"ranking\": \"dense\",\n",
        "        \"search_type\": \"keyword\",\n",
        "        \"processing\": \"summarization\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1500,\n",
        "        \"chunk_overlap\": 300,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"llm\": \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
        "        \"ranking\": \"hybrid\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": \"long_context_reordering\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1500,\n",
        "        \"chunk_overlap\": 300,\n",
        "        \"vector_store\": \"FAISS\",\n",
        "\n",
        "        \"embedding\": \"BAAI/bge-base-en\",\n",
        "        \"llm\": \"llama3-8b-8192\",\n",
        "        \"ranking\": \"BM25\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": None\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1500,\n",
        "        \"chunk_overlap\": 300,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"BAAI/bge-base-en\",\n",
        "        \"llm\": \"qwen-qwq-32b\",\n",
        "        \"ranking\": \"dense\",\n",
        "        \"search_type\": \"keyword\",\n",
        "        \"processing\": \"summarization\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1500,\n",
        "        \"chunk_overlap\": 300,\n",
        "        \"vector_store\": \"FAISS\",\n",
        "\n",
        "        \"embedding\": \"BAAI/bge-base-en\",\n",
        "        \"llm\": \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
        "        \"ranking\": \"hybrid\",\n",
        "        \"search_type\": \"keyword\",\n",
        "        \"processing\": \"long_context_reordering\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1500,\n",
        "        \"chunk_overlap\": 300,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"intfloat/e5-base-v2\",\n",
        "        \"llm\": \"llama3-8b-8192\",\n",
        "        \"ranking\": \"BM25\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": None\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1500,\n",
        "        \"chunk_overlap\": 300,\n",
        "        \"vector_store\": \"FAISS\",\n",
        "\n",
        "        \"embedding\": \"intfloat/e5-base-v2\",\n",
        "        \"llm\": \"qwen-qwq-32b\",\n",
        "        \"ranking\": \"dense\",\n",
        "        \"search_type\": \"keyword\",\n",
        "        \"processing\": \"summarization\"\n",
        "    },\n",
        "    {\n",
        "        \"chunk_size\": 1500,\n",
        "        \"chunk_overlap\": 300,\n",
        "        \"vector_store\": \"Chroma\",\n",
        "\n",
        "        \"embedding\": \"intfloat/e5-base-v2\",\n",
        "        \"llm\": \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
        "        \"ranking\": \"hybrid\",\n",
        "        \"search_type\": \"semantic\",\n",
        "        \"processing\": \"long_context_reordering\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# --- Evaluation Function ---\n",
        "def evaluate_config(config, questions, ground_truths, embedding_model):\n",
        "    \"\"\"Evaluate a single RAG configuration.\"\"\"\n",
        "    text = load_pdf(\"/content/Surah Baqarah tafsir english (2).pdf\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=config[\"chunk_size\"],\n",
        "        chunk_overlap=config[\"chunk_overlap\"]\n",
        "    )\n",
        "    texts = text_splitter.split_text(text)\n",
        "\n",
        "    embedding = HuggingFaceEmbeddings(model_name=config[\"embedding\"])\n",
        "\n",
        "    start_time = time.time()\n",
        "    qa_chain = build_rag_pipeline(texts, config, embedding)\n",
        "    pipeline_time = time.time() - start_time\n",
        "\n",
        "    results = []\n",
        "    for q_idx, question in enumerate(questions):\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            result = qa_chain.invoke({\"query\": question})\n",
        "            time_taken = time.time() - start_time\n",
        "\n",
        "            answer = result[\"result\"]\n",
        "            retrieved_docs = result[\"source_documents\"][:4]\n",
        "            doc_contents = [doc.page_content for doc in retrieved_docs]\n",
        "            gt_answer = ground_truths[q_idx][\"answer\"]\n",
        "\n",
        "            # Calculate all metrics\n",
        "            semantic_sim = calculate_semantic_similarity(gt_answer, answer, embedding_model)\n",
        "            relevance = calculate_relevance_score(answer, doc_contents)\n",
        "            faithfulness = calculate_faithfulness_score(answer, gt_answer)\n",
        "\n",
        "            results.append({\n",
        "                \"question\": question,\n",
        "                \"ground_truth\": gt_answer,\n",
        "                \"generated_answer\": answer,\n",
        "                \"retrieved_docs\": doc_contents,\n",
        "                \"time_taken\": float(time_taken),\n",
        "                \"semantic_similarity\": semantic_sim,\n",
        "                \"relevance_score\": relevance,\n",
        "                \"faithfulness_score\": faithfulness,\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {question}\")\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Aggregate metrics\n",
        "    if results:\n",
        "        avg_metrics = {\n",
        "            \"time_taken\": float(np.mean([r[\"time_taken\"] for r in results])),\n",
        "            \"semantic_similarity\": float(np.mean([r[\"semantic_similarity\"] for r in results])),\n",
        "            \"relevance_score\": float(np.mean([r[\"relevance_score\"] for r in results])),\n",
        "            \"faithfulness_score\": float(np.mean([r[\"faithfulness_score\"] for r in results])),\n",
        "        }\n",
        "    else:\n",
        "        avg_metrics = {\n",
        "            \"time_taken\": 0.0,\n",
        "            \"semantic_similarity\": 0.0,\n",
        "            \"relevance_score\": 0.0,\n",
        "            \"faithfulness_score\": 0.0,\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"config\": config,\n",
        "        \"avg_metrics\": avg_metrics,\n",
        "        \"results\": results\n",
        "    }\n",
        "\n",
        "# --- Main Execution ---\n",
        "# Load data once\n",
        "ground_truths = load_ground_truth(\"surah_baqarah_ground_truth.json\")\n",
        "questions = [q[\"question\"] for q in ground_truths]\n",
        "\n",
        "# Run evaluations\n",
        "all_results = []\n",
        "for config in CONFIGS:\n",
        "    print(f\"\\nTesting: {config}\")\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=config[\"embedding\"])\n",
        "    result = evaluate_config(config, questions, ground_truths, embedding_model)\n",
        "    all_results.append(result)\n",
        "\n",
        "    metrics = result[\"avg_metrics\"]\n",
        "    print(f\"Results:\")\n",
        "    print(f\"- Time: {metrics['time_taken']:.2f}s\")\n",
        "    print(f\"- Semantic Similarity: {metrics['semantic_similarity']:.2f}\")\n",
        "    print(f\"- Relevance: {metrics['relevance_score']:.2f}\")\n",
        "    print(f\"- Faithfulness: {metrics['faithfulness_score']:.2f}\")\n",
        "\n",
        "# Save results\n",
        "with open(\"rag_config_results_3.json\", \"w\") as f:\n",
        "    json.dump(all_results, f, indent=4, cls=NumpyFloatValuesEncoder)\n",
        "print(\"\\n✅ Results saved to 'rag_config_results.json'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5ZWhPfzJNBV"
      },
      "source": [
        "### After evaluating all the models (explaination in pdf report/json/excel), the below configuration is selected for our final RAG pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "sveyjptjTx-v"
      },
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "config = {\n",
        "    \"chunk_size\": 500,\n",
        "    \"chunk_overlap\": 100,\n",
        "    \"vector_store\": \"Chroma\",\n",
        "    \"embedding\": \"BAAI/bge-base-en\",\n",
        "    \"llm\": \"qwen-qwq-32b\",\n",
        "    \"ranking\": \"dense\",\n",
        "    \"search_type\": \"keyword\",\n",
        "    \"processing\": \"summarization\"\n",
        "}\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "htLieoexJur0"
      },
      "outputs": [],
      "source": [
        "# --- LLM Implementation ---\n",
        "class CustomLLM(BaseLLM):\n",
        "    model_name: str = Field(default=\"qwen-qwq-32b\")\n",
        "    temperature: float = Field(default=0.3)\n",
        "    max_tokens: int = Field(default=4000)\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Initialize with either Groq or HuggingFace based on availability\n",
        "        if \"GROQ_API_KEY\" in os.environ:\n",
        "            self._client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "            self._use_groq = True\n",
        "        else:\n",
        "            print(\"Need API Key\")\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        prompts: List[str],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> LLMResult:\n",
        "        try:\n",
        "            if self._use_groq:\n",
        "                response = self._client.chat.completions.create(\n",
        "                    model=self.model_name,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompts[0]}],\n",
        "                    temperature=self.temperature,\n",
        "                    max_tokens=self.max_tokens,\n",
        "                    stop=stop\n",
        "                )\n",
        "                text = response.choices[0].message.content\n",
        "            else:\n",
        "                text = self._client(prompts[0])\n",
        "\n",
        "            return LLMResult(generations=[[{\"text\": text}]])\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling LLM API: {str(e)}\")\n",
        "            return LLMResult(generations=[[{\"text\": \"I don't know\"}]])\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom_llm\"\n",
        "\n",
        "# --- Prompt Template ---\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based ONLY on the following context. If unsure, say \"I don't know\".\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# --- Document Processing ---\n",
        "def load_and_split_documents(file_path):\n",
        "    \"\"\"Load PDF and split documents according to config.\"\"\"\n",
        "    # Load PDF file\n",
        "    reader = PdfReader(file_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "\n",
        "    # Split text according to config\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=config[\"chunk_size\"],\n",
        "        chunk_overlap=config[\"chunk_overlap\"]\n",
        "    )\n",
        "    return text_splitter.split_text(text)\n",
        "\n",
        "\n",
        "# --- Updated Test Pipeline ---\n",
        "def test_pipeline(pipeline, questions):\n",
        "    \"\"\"Test the pipeline with sample questions.\"\"\"\n",
        "    results = []\n",
        "    for question in questions:\n",
        "        print(f\"\\nQuestion: {question}\")\n",
        "\n",
        "        # Get answer\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            result = pipeline.invoke({\"query\": question})\n",
        "            time_taken = time.time() - start_time\n",
        "\n",
        "            # Display results\n",
        "            print(f\"Answer: {result['result']}\")\n",
        "            print(f\"Time taken: {time_taken:.2f}s\")\n",
        "\n",
        "            # Show sources if available\n",
        "            if \"source_documents\" in result:\n",
        "                print(\"\\nTop Sources:\")\n",
        "                for i, doc in enumerate(result[\"source_documents\"][:3]):\n",
        "                    print(f\"{i+1}. {doc.page_content[:200]}...\" if len(doc.page_content) > 200 else f\"{i+1}. {doc.page_content}\")\n",
        "\n",
        "            results.append({\n",
        "                \"question\": question,\n",
        "                \"answer\": result[\"result\"],\n",
        "                \"time_taken\": time_taken,\n",
        "                \"sources\": [doc.page_content for doc in result.get(\"source_documents\", [])]\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {str(e)}\")\n",
        "            results.append({\n",
        "                \"question\": question,\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Pipeline Builder ---\n",
        "def build_pipeline(documents):\n",
        "    \"\"\"Build the complete RAG pipeline.\"\"\"\n",
        "    # Initialize embedding\n",
        "    embedding = HuggingFaceEmbeddings(model_name=config[\"embedding\"])\n",
        "\n",
        "    # Create vector store\n",
        "    if config[\"vector_store\"] == \"Chroma\":\n",
        "        client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "        try:\n",
        "            client.delete_collection(\"rag_collection\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        vectorstore = Chroma.from_texts(\n",
        "            texts=documents,\n",
        "            embedding=embedding,\n",
        "            client=client,\n",
        "            collection_name=\"rag_collection\"\n",
        "        )\n",
        "    else:\n",
        "        vectorstore = FAISS.from_texts(documents, embedding)\n",
        "\n",
        "    # Create retriever based on ranking strategy\n",
        "    if config[\"ranking\"] == \"BM25\":\n",
        "        retriever = BM25Retriever.from_texts(documents, k=4)\n",
        "    elif config[\"ranking\"] == \"dense\":\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "    else:  # Hybrid\n",
        "        bm25_retriever = BM25Retriever.from_texts(documents, k=4)\n",
        "        dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "        retriever = EnsembleRetriever(\n",
        "            retrievers=[bm25_retriever, dense_retriever],\n",
        "            weights=[0.5, 0.5]\n",
        "        )\n",
        "\n",
        "    # Initialize LLM\n",
        "    llm = CustomLLM(\n",
        "        model_name=config[\"llm\"],\n",
        "        temperature=0.3,\n",
        "        max_tokens=4000\n",
        "    )\n",
        "\n",
        "    # Create prompt template\n",
        "    prompt = PromptTemplate(\n",
        "        template=PROMPT_TEMPLATE,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Build QA chain\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    return qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZYrdPVnVWf_",
        "outputId": "bf5fd1b1-1d7d-415b-f0ab-03a750c442c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and processing documents...\n",
            "Loaded 1838 document chunks\n",
            "\n",
            "Building pipeline...\n",
            "\n",
            "Testing pipeline with sample questions...\n",
            "\n",
            "Question: Explain the significance of the cow\n",
            "Answer: \n",
            "<think>\n",
            "Okay, I need to explain the significance of the cow based on the provided context. Let me read through the context again to make sure I understand all the points mentioned.\n",
            "\n",
            "The context starts by describing the cow as bright yellow, not too fat or lean, and must be pleasing to behold—lively and vigorous. Then it talks about Jews making things difficult for themselves by asking about the type of cow they should look for. There's mention of metaphorical language in the Quran about the calf, showing strong love and adoration, which is part of the Quranic style. The Jews are portrayed as God's chosen people but make a big fuss, and there's a reference to them being turned into apes, which was a spiritual state. The main story here is the Episode of the Cow, which is a full account in the Quran. The story involves using a cow to solve a murder: the dead man is restored to life with a piece of the cow. The question raised is why this method was necessary when God could just resurrect without it, and why a cow specifically when they were used to sacrificing cows.\n",
            "\n",
            "So, the significance of the cow in this context seems to be tied to several points. First, the cow has specific physical characteristics that are important. Second, the Jews' insistence on detailed requirements shows their stubbornness or perhaps their testing of God's commands. The metaphor about the calf indicates their strong devotion, but in a negative sense, maybe idolatry? The story of the cow being used to solve the murder case demonstrates a test of faith or obedience. The cow's sacrifice here is a means to an end, perhaps to test the people's obedience or to serve as a lesson. The mention of the Jews being \"chosen people\" but then facing consequences (like being turned into apes spiritually) suggests that their failure to follow correctly leads to punishment. The cow's role in the miracle (restoring the dead) shows that even though God could do it without the cow, He uses this method to test their faith and obedience. The Quranic style uses such stories to highlight lessons for believers.\n",
            "\n",
            "Wait, the context also mentions that the cow's characteristics (yellow, not too fat/lean) are part of the requirements, and the Jews kept asking about specifics, making things hard for themselves. The Episode of the Cow is presented as a story that shows the Israelites' behavior, perhaps their reluctance or excessive questioning. The use of the cow in the ritual to resurrect the dead man's body shows that following the command exactly is necessary, even if the reason isn't clear. The significance might be about obedience, the importance of following divine instructions precisely, and the lesson that their (the Jews') failure to do so leads to negative outcomes. The metaphor of the calf's adoration might refer to idol worship, like the golden calf story, which is a negative example. The cow here is a test of their faith, and their failure to properly follow the instructions (maybe like in the story, someone didn't follow through, like the one who knew but didn't participate) leads to consequences. The story serves as a cautionary tale for believers to adhere strictly to God's commands without unnecessary questioning or making things difficult.\n",
            "\n",
            "Putting this together, the significance of the cow in the Quranic context here is multifaceted: it symbolizes obedience to divine commands, the test of faith through specific rituals, and serves as a lesson against excessive questioning or stubbornness. The detailed requirements of the cow's characteristics emphasize the need for precise adherence. The story also underscores the consequences of failing to follow these instructions, reinforcing the importance of faith and submission. Additionally, the metaphorical references to the calf's adoration might contrast with the proper worship, highlighting idolatry's pitfalls. The use of the cow in the miracle underscores that even when the method seems arbitrary, it's a test of faith, and the Jews' historical examples (like being turned into apes spiritually) show the repercussions of their actions.\n",
            "</think>\n",
            "\n",
            "The significance of the cow in the context provided is tied to several key themes:  \n",
            "\n",
            "1. **Divine Obedience and Specificity**: The cow is described with precise characteristics (bright yellow, not too fat or lean, and \"pleasing to the beholder\"), emphasizing the need for strict adherence to divine instructions. The Jews' repeated questioning about the cow's specifications highlights their tendency to overcomplicate commands, testing their faith and obedience.  \n",
            "\n",
            "2. **Symbol of Trial and Lesson**: The story of the cow serves as a test for the Israelites. Their failure to follow the command fully (e.g., using the cow’s remains to resurrect a murdered man) underscores their reluctance and spiritual shortcomings. This episode acts as a cautionary tale about the consequences of disobedience and excessive skepticism.  \n",
            "\n",
            "3. **Metaphorical Critique**: The mention of the calf metaphor reflects the Quranic critique of idolatry (possibly referencing the golden calf episode in Jewish history), where misplaced devotion (e.g., excessive adoration of symbols) leads to spiritual degradation.  \n",
            "\n",
            "4. **Divine Wisdom in Rituals**: The requirement to use the cow’s remains to solve the murder mystery illustrates that divine commands, even if seemingly arbitrary, have deeper purposes. It challenges humans to trust in God’s wisdom rather than question the \"why\" behind instructions.  \n",
            "\n",
            "Overall, the cow symbolizes obedience, the dangers of spiritual arrogance, and the importance of submitting to divine guidance without unnecessary scrutiny. The story reinforces lessons about faith, accountability, and the consequences of failing to uphold divine commands.\n",
            "Time taken: 3.34s\n",
            "\n",
            "Top Sources:\n",
            "1. of bright yellow colour, not too fat nor too lean, and, most difficult of all, ‘pleasing to the \n",
            "beholder’. This means that the cow must be bright, lively and vigorous; because such are \n",
            "the qualities...\n",
            "2. shown to be forced into their hearts. It is easy to get carried away by this image so a s to \n",
            "almost overlook the real significance of the metaphor used here. It shows their love and \n",
            "adoration for th...\n",
            "3. be physically changed into apes, as the Arabic text may be taken to mean. They had already \n",
            "sunk into that level by thought and spirit. This episode was marked in their history as a \n",
            "useful lesson for...\n",
            "\n",
            "Question: What are the key lessons from the story of Musa?\n",
            "Answer: \n",
            "<think>\n",
            "Okay, let's tackle this question. The user is asking for the key lessons from the story of Musa (Moses) based on the provided context. First, I need to make sure I only use the given context and not any external knowledge.\n",
            "\n",
            "Looking at the context, it mentions a discussion about Jih Ad (maybe a typo, perhaps referring to a surah or chapter in the Quran?) and talks about an important episode from the Israelites after Moses during Prophet David's reign. The context emphasizes that these stories have lessons for Muslims as heirs of Abraham's tradition and their responsibility as world leaders. \n",
            "\n",
            "The text also states that the Quran uses these historical accounts to teach Muslims how to understand and apply its teachings in daily life. The key points here are about learning from the Israelites' mistakes, avoiding the pitfalls that caused earlier communities to lose their leadership and custody of the divine message. \n",
            "\n",
            "Wait, but the question specifically asks about the story of Musa (Moses), but the context provided doesn't directly detail Moses' story. Instead, it refers to an episode after Moses, during David's time. The user might have confused the time periods here. However, the context does mention the Israelites' misguided actions and the need for Muslims to avoid those pitfalls. \n",
            "\n",
            "Since the context doesn't explicitly outline Moses' story's lessons but connects the Israelites' failures to the Muslims' need to learn from them, maybe the key lessons are about heeding God's guidance, avoiding disobedience, and the consequences of straying. The passage also mentions that communities lost their leadership because of their mistakes, so the lessons would include the importance of staying true to divine messages to maintain responsibility and leadership. \n",
            "\n",
            "Additionally, the Quran's purpose here is to educate Muslims in Madinah on applying its teachings practically. So the lessons from the Israelites' stories (even if post-Moses) would be about learning from past errors, adhering to God's commands, and the responsibility that comes with being a chosen community. \n",
            "\n",
            "I need to ensure that I don't mention David's story directly since the question is about Moses, but the context links the Israelites' overall history. The answer should focus on the lessons derived from their experiences as presented in the context, even if it's not Moses' specific story. The main points are avoiding the pitfalls of previous communities, the importance of heeding messages to retain leadership, and applying Quranic teachings practically.\n",
            "</think>\n",
            "\n",
            "The key lessons from the story of Musa (Moses) as implied in the context are derived from the broader narrative of the Israelites' experiences, which serve as a cautionary example for Muslims. The text emphasizes that the Israelites, despite receiving divine guidance, fell into errors and \"pitfalls,\" leading to their loss of \"custody of the divine message and leadership of mankind.\" This underscores the importance of learning from their mistakes to avoid repeating them. The context highlights that Muslims, as successors to Abraham’s tradition, must heed God’s commands, remain steadfast in their faith, and fulfill their responsibility as leaders by adhering to divine guidance. Additionally, the Quran’s teachings aim to equip Muslims with the wisdom to interpret its messages practically, ensuring they do not succumb to the failures of earlier communities. The central lessons include accountability for leadership, the consequences of disobedience, and the need for continuous moral and spiritual vigilance.\n",
            "Time taken: 2.27s\n",
            "\n",
            "Top Sources:\n",
            "1. discussion on jih ad, citing an important episode from the hist ory of the Israelites after \n",
            "Moses, during the reign of the Prophet David, which has many essential lessons for the \n",
            "Muslims as heirs of...\n",
            "2. Human history is vastly rich in accounts of human endeavour, and the tests and \n",
            "tribulations people of different ages and generations encountered. The personalities and \n",
            "the details of th ose events t...\n",
            "3. avoid the pitfalls into which earlier recipients of God’s messa ge, had fallen. As a result, \n",
            "such communities were stripped of the honour of having custody of the divine message \n",
            "and of the leadershi...\n",
            "\n",
            "Pipeline testing complete!\n",
            "\n",
            "Test Results Summary:\n",
            "\n",
            "Question 1: Explain the significance of the cow\n",
            "Answer: \n",
            "<think>\n",
            "Okay, I need to explain the significance of the cow based on the provided context. Let me read through the context again to make sure I understand all the points mentioned.\n",
            "\n",
            "The context start...\n",
            "Time: 3.34s\n",
            "\n",
            "Question 2: What are the key lessons from the story of Musa?\n",
            "Answer: \n",
            "<think>\n",
            "Okay, let's tackle this question. The user is asking for the key lessons from the story of Musa (Moses) based on the provided context. First, I need to make sure I only use the given context ...\n",
            "Time: 2.27s\n"
          ]
        }
      ],
      "source": [
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample document and questions\n",
        "    document_path = \"/content/Surah Baqarah tafsir english (2).pdf\"\n",
        "    sample_questions = [\n",
        "        \"Explain the significance of the cow\",\n",
        "        \"What are the key lessons from the story of Musa?\"\n",
        "    ]\n",
        "\n",
        "    # Build and test pipeline\n",
        "    print(\"Loading and processing documents...\")\n",
        "    try:\n",
        "        documents = load_and_split_documents(document_path)\n",
        "        print(f\"Loaded {len(documents)} document chunks\")\n",
        "\n",
        "        print(\"\\nBuilding pipeline...\")\n",
        "        pipeline = build_pipeline(documents)\n",
        "\n",
        "        print(\"\\nTesting pipeline with sample questions...\")\n",
        "        test_results = test_pipeline(pipeline, sample_questions)\n",
        "\n",
        "        print(\"\\nPipeline testing complete!\")\n",
        "\n",
        "        # Print summary of results\n",
        "        print(\"\\nTest Results Summary:\")\n",
        "        for i, result in enumerate(test_results):\n",
        "            print(f\"\\nQuestion {i+1}: {result['question']}\")\n",
        "            if 'answer' in result:\n",
        "                print(f\"Answer: {result['answer'][:200]}...\" if len(result['answer']) > 200 else f\"Answer: {result['answer']}\")\n",
        "                print(f\"Time: {result['time_taken']:.2f}s\")\n",
        "            else:\n",
        "                print(f\"Error: {result['error']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Pipeline failed: {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
